{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#google colab\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import json\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "a29GnZIfdeuL",
        "cell_id": "0f32548964af44f1ba0acd4f96cb6469",
        "source_hash": "48f79535",
        "execution_start": 1685014006712,
        "execution_millis": 1,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wzrqvryVd0Pf",
        "cell_id": "8829776d0c234158bdff1607566571f3",
        "source_hash": "5274cba9",
        "execution_start": 1685014007693,
        "execution_millis": 9,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "outputId": "f3bb1588-09bb-48c2-f7db-f80c4bfaf8d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# Mappings for utility\n",
        "MAP_PERCENT_TO_AMOUNT = {\n",
        "    \"10\" : 280,\n",
        "    \"25\" : 700,\n",
        "    \"50\" : 1400,\n",
        "    \"100\": 2799\n",
        "}\n",
        "\n",
        "DAMAGE_LEVEL_TO_SCORE = {\n",
        "    \"destroyed\" : 4,\n",
        "    \"major-damage\" : 3,\n",
        "    \"minor-damage\" : 2,\n",
        "    \"no-damage\" : 1,\n",
        "}\n",
        "\n",
        "PERCENT = \"100\"\n",
        "TRAIN_SET_SIZE = MAP_PERCENT_TO_AMOUNT[PERCENT]"
      ],
      "metadata": {
        "id": "DjW1R0s2deuN",
        "cell_id": "e2b43ba5f0f84e4083e4a0eedf854b3e",
        "source_hash": "23722508",
        "execution_start": 1685014008928,
        "execution_millis": 2,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "id": "3s-OW6V0eK13",
        "cell_id": "7acec0c7b1fb436c8b6a05cf1f818aa0",
        "source_hash": "99b2ca84",
        "execution_start": 1685014011476,
        "execution_millis": 5,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "outputId": "f95988f4-2af8-4a0f-fe72-32d1084bc831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'drive', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_FOLDER_PATH = \"./drive/MyDrive/HTCV-Sem/dataset/\"\n",
        "XVIEW2_TXT_FILE = \"./drive/MyDrive/HTCV-Sem/dataset//xview2.txt\"\n",
        "MODEL_OUTPUT_FILENAME = \"model_deepnote_test\""
      ],
      "metadata": {
        "id": "PN0-MvlcdeuO",
        "cell_id": "632f5d87229f409c874ee69c8969988d",
        "source_hash": "5258c29a",
        "execution_start": 1685014043874,
        "execution_millis": 2,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /datasets/xviewdataset/HTCV-Sem/dataset/"
      ],
      "metadata": {
        "cell_id": "dc74046fea604b12b754d65ef62238cb",
        "source_hash": "e4744247",
        "execution_start": 1685014012884,
        "execution_millis": 687,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "4nWWuh5uxSAo",
        "outputId": "386f9c46-3eac-42a4-94ba-e22b854b0437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/datasets/xviewdataset/HTCV-Sem/dataset/': No such file or directory\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Baseline model\n",
        "class ResNetClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetClassifier, self).__init__()\n",
        "        self.resnet = models.resnet18(weights=None)\n",
        "        self.resnet.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        return x\n",
        "    \n",
        "    def train_model(self, dataloader, num_epochs, lr, momentum):\n",
        "        self.train()\n",
        "        self.to(device)\n",
        "\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr, momentum)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        # Train the model\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for images, labels in dataloader:\n",
        "                images, labels =  images.to(device),labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass and optimizatio.n\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
        "        torch.save(self.state_dict(), MODEL_OUTPUT_FILENAME + '.pth')\n",
        "    # Define the evaluation function\n",
        "    def evaluate_model(self, test_dataloader):\n",
        "        self.eval()  # Set the model to evaluation mode\n",
        "        self.to(device)\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        true_labels = []\n",
        "        predicted_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_dataloader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = self(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "                predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "        accuracy = correct / total\n",
        "        confusion_mat = confusion_matrix(true_labels, predicted_labels, normalize=\"true\")\n",
        "\n",
        "\n",
        "        return accuracy, confusion_mat"
      ],
      "metadata": {
        "id": "DlES_LUSdeuO",
        "cell_id": "248807e634d34b35a576fe10386c0912",
        "source_hash": "bdbb3d8a",
        "execution_start": 1685014013607,
        "execution_millis": 6,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predefined training samples for the seminar\n",
        "def get_training_set_entries(amount=280):\n",
        "    xview_file = open(XVIEW2_TXT_FILE,'r')\n",
        "    return xview_file.read().splitlines()[:amount]"
      ],
      "metadata": {
        "id": "_wHERJXgdeuP",
        "cell_id": "cc7830f0ed524fb093079e3fc14693a2",
        "source_hash": "68cfcd9a",
        "execution_start": 1685014015616,
        "execution_millis": 4,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "# Get mean and standard deviation of dataset\n",
        "# This is needed for normalization\n",
        "def calc_mean_and_std_of_dataset(loader):\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        batch_samples = images.size(0)\n",
        "        images = images.view(batch_samples, images.size(1), -1)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "        total_samples += batch_samples\n",
        "\n",
        "    mean /= total_samples\n",
        "    std /= total_samples\n",
        "    \n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "l0H_7LJudeuP",
        "cell_id": "52e63fff2fc24c48a5ee9615d3018933",
        "source_hash": "1843f377",
        "execution_start": 1685014017424,
        "execution_millis": 5,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize Dataset"
      ],
      "metadata": {
        "id": "J1eCj5HAdeuP",
        "cell_id": "e2287b8ce4524fc6a194ef5f774d0103",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess your dataset using torchvision.transforms\n",
        "pre_norm_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the input image to match the expected size of ResNet (224x224)\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "# Calculate the mean and standard deviation of your dataset\n",
        "path_to_dataset = os.path.join(DATASET_FOLDER_PATH,\"train\")\n",
        "dataset = torchvision.datasets.ImageFolder(path_to_dataset, transform=pre_norm_transform)\n",
        "data_point_names = get_training_set_entries(amount = TRAIN_SET_SIZE)\n",
        "\n",
        "subset = [dataset[dataset.imgs.index((file_path, class_label))] for file_path, class_label in dataset.imgs if os.path.basename(file_path) in data_point_names]\n",
        "loader = torch.utils.data.DataLoader(subset, batch_size=32, shuffle=False)\n",
        "\n",
        "mean, std = calc_mean_and_std_of_dataset(loader=loader)"
      ],
      "metadata": {
        "id": "fn5BpbUddeuQ",
        "cell_id": "7409b7bad3714c94b5aa73472a419d99",
        "source_hash": "6f7bea66",
        "execution_start": 1685014049020,
        "execution_millis": 120,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "outputId": "23401a3b-cf61-4e8d-948e-36cea33498bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-dfed30e1f774>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata_point_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_set_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAIN_SET_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_point_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-dfed30e1f774>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata_point_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_set_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAIN_SET_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_point_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2982\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2984\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup training model"
      ],
      "metadata": {
        "id": "8pOMod89deuQ",
        "cell_id": "17bd5b27779347e7ac8880fbdbad0bed",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Update the normalization transform using your dataset's mean and standard deviation\n",
        "transform = transforms.Compose([\n",
        "    pre_norm_transform,\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Example dataset loading using torchvision.datasets.ImageFolder\n",
        "dataset = torchvision.datasets.ImageFolder(path_to_dataset, transform=transform)\n",
        "subset = [dataset[dataset.imgs.index((file_path, class_label))] for file_path, class_label in dataset.imgs if os.path.basename(file_path) in data_point_names]\n",
        "dataloader = torch.utils.data.DataLoader(subset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "ETDKZS-ideuQ",
        "cell_id": "897e0dfb152a41f18894c41f3caa49ea",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "rTzIxsmYdeuR",
        "cell_id": "9282d71c742041f4a6f3d9030786d723",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_classes = 5  # Number of output classes\n",
        "\n",
        "num_epochs=50\n",
        "lr=0.01\n",
        "momentum=0.9\n",
        "model = ResNetClassifier(num_classes)\n",
        "\n",
        "model.train_model(dataloader, num_epochs, lr, momentum)\n"
      ],
      "metadata": {
        "id": "HmA8mz_UdeuR",
        "cell_id": "c4316131f052442fb2ceebc7234155d6",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup evaluating test performance"
      ],
      "metadata": {
        "id": "Il1MBF6SdeuR",
        "cell_id": "233aca035ae643a7b181b614e329ff65",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = torchvision.datasets.ImageFolder(os.path.join(DATASET_FOLDER_PATH, \"test\"), transform=transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False,num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "c02HwsKUdeuR",
        "cell_id": "a8eb364cd1034306bce244d97fe24cd3",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate model on test data"
      ],
      "metadata": {
        "id": "3Llj15MBdeuS",
        "cell_id": "c14c7a11032945bd90792af8505e1fd7",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LOAD_MODEL = False\n",
        "LOAD_MODEL_PATH = 'resnet_classifier.pth'\n",
        "#Load the saved model state dictionary\n",
        "if(LOAD_MODEL):\n",
        "  model = ResNetClassifier(num_classes)\n",
        "  model.load_state_dict(torch.load('resnet_classifier.pth'))\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy, confusion_mat = model.evaluate_model(test_dataloader=test_dataloader)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n"
      ],
      "metadata": {
        "id": "nsDTzwi8deuS",
        "cell_id": "f59fb3d01700449e96d9a1a8f3d350de",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "mean_val = [mean_i.item() for mean_i in mean]\n",
        "std_val = [std_i.item() for std_i in std]\n",
        "data_info = {\n",
        "    'dataset_size' : MAP_PERCENT_TO_AMOUNT[PERCENT],\n",
        "    'mean_0' : mean.[0],\n",
        "    'mean_1' : mean_val[1],\n",
        "    'mean_2' : mean_val[2],\n",
        "    'standard_dev_0' : std_val[0],\n",
        "    'standard_dev_1' : std_val[1],\n",
        "    'standard_dev_2' : std_val[2]\n",
        "}\n",
        "model_info = {\n",
        "    'dataset_size' : MAP_PERCENT_TO_AMOUNT[PERCENT],\n",
        "    'architecture' : \"resnet18\",\n",
        "    'learning_rate': lr,\n",
        "    'batch_size': 32,\n",
        "    'num_epochs': num_epochs,\n",
        "    'momentum' : momentum,\n",
        "    'accuracy' : accuracy,\n",
        "    'confusion_mat' : confusion_mat.tolist()\n",
        "}\n",
        "with open(\"dataset_size_\" + str(MAP_PERCENT_TO_AMOUNT[PERCENT]) +'.json', 'w') as file:\n",
        "    json.dump(data_info, file)\n",
        "\n",
        "with open(MODEL_OUTPUT_FILENAME +'.json', 'w') as file:\n",
        "    json.dump(model_info, file)"
      ],
      "metadata": {
        "id": "TGljGznc5w8g",
        "cell_id": "7df6ff2928964b7cb9b0ca8dc9870db1",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Results"
      ],
      "metadata": {
        "id": "dBxQcEc-deuS",
        "cell_id": "f0d3346ff81d4b66a2ba95bc7ffc1166",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class labels\n",
        "class_labels = test_dataset.classes\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "sns.heatmap(confusion_mat, annot=True, fmt=\".3f\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=class_labels, yticklabels=class_labels, ax=ax)\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel(\"Predicted Labels\")\n",
        "ax.set_ylabel(\"True Labels\")\n",
        "ax.set_title(\"Confusion Matrix\")\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wFmSOI-ideuS",
        "cell_id": "16e7c8fa18914e4c94f8ca460a0601f2",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ac044641-96a0-4e06-ae59-4421e551585a' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ],
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "QWQszVILxSAu"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "deepnote": {},
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python"
    },
    "orig_nbformat": 4,
    "deepnote_notebook_id": "f7a42fbfe0fd4b20ae96bc3c8d717607",
    "deepnote_execution_queue": []
  }
}